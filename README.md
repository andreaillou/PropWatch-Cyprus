# PropWatch Cyprus

Computational pipeline for a two-paper academic project studying how Russian
foreign-broadcast propaganda amplifies pre-existing Cypriot social cleavages.

**Cleavage codes (Layer 2 annotation schema):** Cyprus division (`CY-DIV`),
EU scepticism (`EU-SKEP`), 2013 bail-in (`BAIL-IN`), Orthodox identity
(`ORTHO`), anti-elite populism (`ELIT`), migration (`MIGR`).

**Paper 1 â€” Propaganda techniques & narrative framing:**
SemEval-2020 14-class technique classification (XLM-RoBERTa-large) +
BERTopic narrative clustering with temporal analysis (H1, H4).

**Paper 2 â€” Amplification dynamics:**
Interrupted time series (ITS) around the January 2026 kompromat event;
`forwards` is the primary amplification proxy (H3).

Both papers share one corpus scraped from the Russian Embassy Cyprus
channel (`rusembcy`) on Telegram.

## Repository structure

```
â”œâ”€â”€ main.ipynb                     # End-to-end pipeline notebook
â”œâ”€â”€ requirements.txt               # Python dependencies (see sections)
â”œâ”€â”€ .env.example                   # Template for API credentials
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ channels.yaml              # Telegram channel source list (by tier)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                  # All file paths and scraping constants
â”‚   â”œâ”€â”€ scraping/
â”‚   â”‚   â”œâ”€â”€ telegram.py            # Telethon scraper â€” collects message_id,
â”‚   â”‚   â”‚                          #   views, forwards, reactions, reply_to_id,
â”‚   â”‚   â”‚                          #   edit_date alongside date/channel/text
â”‚   â”‚   â”œâ”€â”€ twitter.py             # twarc2 scraper â€” Jan 2026 kompromat event
â”‚   â”‚   â””â”€â”€ gdelt.py               # GDELT + Wayback scraper â€” Tier 1 archived
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ filtering.py           # 4-step pipeline: length â†’ spam â†’ topic
â”‚   â”‚   â”‚                          #   keywords â†’ dedup; tags 9 binary columns
â”‚   â”‚   â”‚                          #   (3 existing + 6 cleavage codes)
â”‚   â”‚   â””â”€â”€ text_cleaning.py       # Text normalisation; lingua-py language
â”‚   â”‚                              #   detection (authoritative); script-type
â”‚   â”‚                              #   heuristic (secondary); splits corpus
â”‚   â”‚                              #   into russian / english / greek subsets
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ lemmatization.py       # stanza-based lemmatization for Russian
â”‚   â”‚   â”‚                          #   (ru pipeline) and Greek (el pipeline);
â”‚   â”‚   â”‚                          #   spaCy is NOT used here
â”‚   â”‚   â””â”€â”€ frequency.py           # Word frequency & n-gram analysis
â”‚   â”‚                              #   (script-agnostic, works on list[str])
â”‚   â””â”€â”€ classification/
â”‚       â””â”€â”€ model.py               # Propaganda classifier stub (TODO)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ telegram/              # Per-channel raw CSVs
â”‚   â”‚   â””â”€â”€ twitter/               # Twitter raw CSVs
â”‚   â””â”€â”€ processed/                 # Merged corpus and analysis outputs
â””â”€â”€ models/                        # Trained model weights (not committed)
```

## Quick-start

```bash
# 1. Clone and install
git clone <repo-url>
cd PropWatch-Cyprus
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Download stanza models (lemmatization â€” run once)
python -c "import stanza; stanza.download('ru'); stanza.download('el')"

# 3. Download spaCy English model (Track A syntactic features â€” run once)
python -m spacy download en_core_web_lg

# 4. Configure Telegram credentials
cp .env.example .env
# Edit .env â€” set TELEGRAM_APP_ID and TELEGRAM_API_HASH

# 5. Run the notebook
jupyter notebook main.ipynb
# â€” or run the scraper directly â€”
python -m src.scraping.telegram

# 6. (Optional) Twitter scraping â€” requires bearer token in .env
python -m src.scraping.twitter

# 7. (Optional) Tier 1 archived content â€” RT/Sputnik via GDELT + Wayback
python -m src.scraping.gdelt
# Note: fetch_text=True is slow (~1.5s per article). For metadata-only
# discovery pass, edit gdelt.py and set fetch_text=False first.
```

## Pipeline overview

```
scrape_channels()          â†’ raw CSV (9 fields incl. forwards, reactions)
    â†“
filter_messages()          â†’ length / spam / topic filter + 9 binary tags
    â†“
clean_and_split()          â†’ text_cleaned | script_type | language
                             â””â”€ russian_df / english_df / greek_df
    â†“
lemmatize_column()         â†’ Russian lemmas (stanza ru)
lemmatize_greek_column()   â†’ Greek lemmas  (stanza el)
    â†“
word_frequency()           â†’ per-language top-N lemma frequency CSVs
compute_ngrams()           â†’ bigrams / trigrams per language
    â†“
[TODO] BERTopic            â†’ narrative clusters + temporal drift (H1, H4)
[TODO] XLM-RoBERTa-large  â†’ SemEval-2020 14-class technique labels (Paper 1)
[TODO] ITS regression      â†’ amplification analysis around Jan 2026 (H3)
```

## Status

| Component | Status |
| :-- | :-- |
| Telegram scraping (single channel) | âœ… Working |
| Multi-channel config (`channels.yaml`) | ðŸš§ Added â€” populate Tier 2 handles from source list |
| Twitter/X scraper (Jan 2026 kompromat) | ðŸš§ Added â€” requires bearer token |
| Tier 1 archived scraper (GDELT + Wayback + trafilatura) | ðŸš§ Added â€” `python -m src.scraping.gdelt` |
| Keyword filtering & cleavage-code tagging | âœ… Working |
| lingua-py language detection | âœ… Working |
| Text cleaning & corpus split | âœ… Working (bug fix applied) |
| Russian lemmatization (stanza) | âœ… Working |
| Greek lemmatization (stanza) | âœ… Working |
| Frequency / n-gram analysis | âœ… Working |
| BERTopic narrative clustering | ðŸš§ TODO â€” H1 / H4 |
| XLM-RoBERTa-large classification | ðŸš§ TODO â€” awaiting fine-tuned weights |
| Interrupted time series (H3) | ðŸš§ TODO â€” Jan 2026 kompromat event |
